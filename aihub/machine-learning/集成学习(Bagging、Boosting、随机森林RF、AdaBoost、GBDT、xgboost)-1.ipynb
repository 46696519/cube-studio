{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可以通过聚集多个分类器的预测结果提高分类器的分类准确率**，这一方法称为集成（Ensemble）学习或分类器组合（Classifier Combination）\n",
    "\n",
    "该方法由训练数据构建一组基分类器（Base Classifier），然后通过对每个基分类器的预测进行投票来进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成学习（ensemble learning）通过组合多个基分类器（base classifier）来完成学习任务，颇有点“三个臭皮匠顶个诸葛亮”的意味。\n",
    "\n",
    "基分类器一般采用的是弱可学习（weakly learnable）分类器，通过集成学习，组合成一个强可学习（strongly learnable）分类器。\n",
    "\n",
    ">所谓弱可学习，是指学习的正确率仅略优于随机猜测的多项式学习算法；强可学习指正确率较高的多项式学习算法。\n",
    "\n",
    "集成学习的泛化能力一般比单一的基分类器要好，**这是因为大部分基分类器都分类错误的概率远低于单一基分类器的**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 偏差与方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“偏差-方差分解”（bias variance decomposition）是用来解释机器学习算法的泛化能力的一种重要工具。\n",
    "\n",
    "对于同一个算法，在不同训练集上学得结果可能不同。\n",
    "\n",
    "对于训练集$D={(x_1 ,y_1 ),(x_2 ,y_2 ),⋯,(x_N ,y_N )} $\n",
    "\n",
    "由于噪音，样本$x$的真实类别为$y$（在训练集中的类别为$y$），则噪声为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ξ^2 =E_D [(y_d −y)^2 ] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种学习模型得到的预测值的期望为\n",
    "\n",
    "$$\\hat{f(x)}=E_D [f(x;D)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用样本数相同的不同训练集所产生的方差\n",
    "$$var(x)=E_D [(f(x;D)−\\hat{f(x)}) ^2 ] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "期望输出与真实类别的差别称为bias，则\n",
    "\n",
    "$$bias^2 (x)=(\\hat{f (x)}−y)^2  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging（Bootstrap Aggregating）对训练数据采用自助采样（boostrap sampling），即有放回地采样数据；\n",
    "\n",
    "每一次的采样数据集训练出一个基分类器，经过M次采样得到M个基分类器，然后根据最大表决（majority vote）原则组合基分类器的分类结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/c1a2c54b154dabb80bb8d03c3840e6f8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "Boosting的思路则是采用重赋权（re-weighting）法迭代地训练基分类器，即对每一轮的训练数据样本赋予一个权重，并且每一轮样本的权值分布依赖上一轮的分类结果；\n",
    "\n",
    "基分类器之间采用序列式的线性加权方式进行组合。他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost， GBDT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/3d72b65cd38540082ef4765d67859d75.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从“偏差-方差分解”的角度看，Bagging关注于降低variance，而Boosting则是降低bias；\n",
    "\n",
    "Boosting的基分类器是强相关的，并不能显著降低variance。Bagging与Boosting有分属于自己流派的两大杀器：\n",
    "\n",
    "随机森林Random Forests（RF）和梯度下降树Gradient Boosting Decision Tree（GBDT）。\n",
    "\n",
    "AdaBoost属于Boosting流派。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging、Boosting二者之间的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本选择上：\n",
    "\n",
    ">Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。\n",
    "\n",
    ">Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。\n",
    "\n",
    "样例权重：\n",
    "\n",
    ">Bagging：使用均匀取样，每个样例的权重相等。\n",
    "\n",
    ">Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。\n",
    "\n",
    "预测函数：\n",
    "\n",
    ">Bagging：所有预测函数的权重相等。\n",
    "\n",
    ">Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。\n",
    "\n",
    "并行计算：\n",
    "\n",
    ">Bagging：各个预测函数可以并行生成。\n",
    "\n",
    ">Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。\n",
    "\n",
    "**下面是将决策树与这些算法框架进行结合所得到的新的算法：**\n",
    "\n",
    ">Bagging + 决策树 = 随机森林\n",
    "\n",
    ">AdaBoost + 决策树 = 提升树\n",
    "\n",
    ">Gradient Boosting + 决策树 = GBDT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
